{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grupo 3 del proyecto de data mining - Analisis de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Authors : H. Kanza, N.Matte, J. Escrihuela, A. Ortiz, ...***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import requests\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from bs4 import BeautifulSoup\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Import data from the website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I download the HTML page where I can find the URLs of every monthly file I want to analyse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved at app/data/webpage_omie.html\n"
     ]
    }
   ],
   "source": [
    "# URL of the file I want to download\n",
    "url_page = \"https://www.omie.es/en/file-access-list?parents%5B0%5D=/&parents%5B1%5D=Day-ahead%20Market&parents%5B2%5D=3.%20Curves&dir=Monthly%20files%20with%20aggregate%20supply%20and%20demand%20curves%20of%20Day-ahead%20market%20including%20bid%20units&realdir=curva_pbc_uof\"\n",
    "\n",
    "# Path where I want to save the file\n",
    "path = \"app/data/webpage_omie.html\"\n",
    "\n",
    "# GET request to try to access URL data\n",
    "response = requests.get(url_page)\n",
    "\n",
    "# I make sure the request succeeded\n",
    "if response.status_code == 200:\n",
    "    # ... so I write the data in a local file, in the specified path\n",
    "    with open(path, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    print(f\"File saved at {path}\")\n",
    "else:\n",
    "    print(f\"Request error code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with the HTML, I need to extract the list containing the URLs of all curves files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76 occurences found on the webpage.\n"
     ]
    }
   ],
   "source": [
    "# I read the HTML file\n",
    "with open(path, \"r\", encoding=\"utf-8\") as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "    # Pattern with characters before and after\n",
    "    pattern = r\".*curva_pbc_uof_*.\"\n",
    "\n",
    "    # Find occurrences that match the pattern\n",
    "    matches = re.findall(pattern, html_content)\n",
    "\n",
    "    if matches:\n",
    "        print(len(matches), \"occurences found on the webpage.\")\n",
    "    else:\n",
    "        print(\"No occurence found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descargar_archivos_ultimos_3_meses(mes, año):\n",
    "    # Calcular los últimos 3 meses\n",
    "    fechas = []\n",
    "    for i in range(3):\n",
    "        fecha = datetime(year=año, month=mes, day=1) - timedelta(days=30 * i)\n",
    "        fechas.append(fecha.strftime('%Y%m'))\n",
    "\n",
    "    # URL base\n",
    "    url_base = \"https://www.omie.es/en/file-access-list?parents%5B0%5D=/&parents%5B1%5D=Intraday%20Auction%20Market&parents%5B2%5D=3.%20Curves&dir=Monthly%20files%20with%20aggregate%20supply%20and%20demnand%20curves%20of%20intraday%20auction%20market%20including%20bid%20units&realdir=curva_pibc_uof\"\n",
    "\n",
    "    # Hacer una solicitud GET para obtener el contenido de la página\n",
    "    response = requests.get(url_base)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        # Parsear el contenido HTML\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Buscar todos los enlaces en la página que coincidan con el patrón de nombre del archivo\n",
    "        links = soup.find_all('a', href=True)\n",
    "        file_names = [re.search(r'curva_pibc_uof_\\d{6}.zip', link['href']).group(0) for link in links if re.search(r'curva_pibc_uof_\\d{6}.zip', link['href'])]\n",
    "\n",
    "        if file_names:\n",
    "            # Filtrar los archivos por los últimos 3 meses\n",
    "            archivos_a_descargar = [file for file in file_names if any(fecha in file for fecha in fechas)]\n",
    "\n",
    "            if not archivos_a_descargar:\n",
    "                print(\"No se encontraron archivos para los últimos 3 meses especificados.\")\n",
    "                return\n",
    "\n",
    "            # Descargar cada archivo\n",
    "            for archivo in archivos_a_descargar:\n",
    "                # Construir la URL completa del archivo\n",
    "                latest_file_url = f\"https://www.omie.es/en/file-download?parents%5B0%5D=curva_pibc_uof&filename={archivo}\"\n",
    "                # Nombre del archivo a guardar en el disco\n",
    "                file_name = f\"app/data/{archivo}\"\n",
    "\n",
    "                # Hacer una solicitud GET para descargar el archivo ZIP\n",
    "                response = requests.get(latest_file_url)\n",
    "\n",
    "                if response.status_code == 200:\n",
    "                    # Guardar el archivo ZIP en la ruta especificada\n",
    "                    os.makedirs(os.path.dirname(file_name), exist_ok=True)\n",
    "                    with open(file_name, \"wb\") as f:\n",
    "                        for chunk in response.iter_content(chunk_size=8192):\n",
    "                            f.write(chunk)\n",
    "                    print(f\"Descarga completa: {file_name}\")\n",
    "                else:\n",
    "                    print(f\"Error al descargar el archivo: {response.status_code}\")\n",
    "        else:\n",
    "            print(\"No se encontraron archivos que coincidan con el patrón especificado.\")\n",
    "    else:\n",
    "        print(f\"Error en la solicitud a la página: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descarga completa: data/curva_pibc_uof_202402.zip\n",
      "Descarga completa: data/curva_pibc_uof_202401.zip\n",
      "Descarga completa: data/curva_pibc_uof_202312.zip\n"
     ]
    }
   ],
   "source": [
    "descargar_archivos_ultimos_3_meses(2, 2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extraer_archivos_zip(directorio_origen, directorio_destino):\n",
    "    # Crear el directorio destino si no existe\n",
    "    if not os.path.exists(directorio_destino):\n",
    "        os.makedirs(directorio_destino)\n",
    "\n",
    "    # Obtener la lista de archivos ZIP en el directorio de origen\n",
    "    archivos_zip = [archivo for archivo in os.listdir(directorio_origen) if archivo.endswith('.zip')]\n",
    "\n",
    "    for archivo in archivos_zip:\n",
    "        file_path = os.path.join(directorio_origen, archivo)\n",
    "        archivo_descomprimido = os.path.join(directorio_destino, archivo[:-4])\n",
    "\n",
    "        # Comprobar si el archivo ya ha sido descomprimido\n",
    "        if not os.path.exists(archivo_descomprimido):\n",
    "            with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall(directorio_destino)\n",
    "            print(f\"Archivo {file_path} descomprimido en {directorio_destino}\")\n",
    "        else:\n",
    "            print(f\"El archivo {file_path} ya ha sido descomprimido en {directorio_destino}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo app/data/curva_pibc_uof_202301.zip descomprimido en descomprimido/\n",
      "Archivo app/data/curva_pibc_uof_202303.zip descomprimido en descomprimido/\n",
      "Archivo app/data/curva_pibc_uof_202304.zip descomprimido en descomprimido/\n",
      "Archivo app/data/curva_pibc_uof_202312.zip descomprimido en descomprimido/\n",
      "Archivo app/data/curva_pibc_uof_202401.zip descomprimido en descomprimido/\n",
      "Archivo app/data/curva_pibc_uof_202402.zip descomprimido en descomprimido/\n",
      "Archivo app/data/first_file.zip descomprimido en descomprimido/\n"
     ]
    }
   ],
   "source": [
    "# Llamada a la función para extraer archivos ZIP\n",
    "directorio_origen = 'app/data/'\n",
    "directorio_destino = 'descomprimido/'\n",
    "extraer_archivos_zip(directorio_origen, directorio_destino)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener la lista de archivos descomprimidos\n",
    "archivos_descomprimidos = os.listdir(directorio_destino)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cambiar el nombre de los archivos .1 a .csv\n",
    "for archivo in archivos_descomprimidos:\n",
    "    if archivo.endswith('.1'):\n",
    "        # Construir las rutas de origen y destino\n",
    "        ruta_origen = os.path.join(directorio_destino, archivo)\n",
    "        ruta_destino = os.path.join(directorio_destino, archivo[:-2] + '.csv')\n",
    "        # Verificar si el archivo .csv ya existe\n",
    "        if not os.path.exists(ruta_destino):\n",
    "            # Renombrar el archivo\n",
    "            os.rename(ruta_origen, ruta_destino)\n",
    "            #print(f\"Renombrado {ruta_origen} a {ruta_destino}\")\n",
    "        #else:\n",
    "            #print(f\"El archivo {ruta_destino} ya existe y no se renombrará.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener la lista de archivos CSV en el directorio\n",
    "archivos_descomprimidos = [f for f in os.listdir(directorio_destino) if f.endswith('.csv')]\n",
    "\n",
    "# Inicializar un DataFrame vacío para almacenar los datos combinados\n",
    "data_frame_combinado = pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterar sobre los archivos CSV y leerlos en el DataFrame\n",
    "for archivo in archivos_descomprimidos:\n",
    "    ruta_archivo = os.path.join(directorio_destino, archivo)\n",
    "    # Leer el archivo saltando las dos primeras filas y usando la tercera fila como encabezado\n",
    "    datos_archivo = pd.read_csv(ruta_archivo, skiprows=2,delimiter=';', encoding=\"latin-1\")\n",
    "    # Concatenar los datos al DataFrame combinado\n",
    "    data_frame_combinado = pd.concat([data_frame_combinado, datos_archivo], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hora</th>\n",
       "      <th>Fecha</th>\n",
       "      <th>Pais</th>\n",
       "      <th>Unidad</th>\n",
       "      <th>Tipo Oferta</th>\n",
       "      <th>Energ�a Compra/Venta</th>\n",
       "      <th>Precio Compra/Venta</th>\n",
       "      <th>Ofertada (O)/Casada (C)</th>\n",
       "      <th>Unnamed: 8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>01/12/2023</td>\n",
       "      <td>MI</td>\n",
       "      <td>WMVD204</td>\n",
       "      <td>C</td>\n",
       "      <td>2,2</td>\n",
       "      <td>1.002,00</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>01/12/2023</td>\n",
       "      <td>MI</td>\n",
       "      <td>WMVD090</td>\n",
       "      <td>C</td>\n",
       "      <td>0,1</td>\n",
       "      <td>1.002,00</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>01/12/2023</td>\n",
       "      <td>MI</td>\n",
       "      <td>NXVD238</td>\n",
       "      <td>C</td>\n",
       "      <td>0,8</td>\n",
       "      <td>700,00</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>01/12/2023</td>\n",
       "      <td>MI</td>\n",
       "      <td>NXVD253</td>\n",
       "      <td>C</td>\n",
       "      <td>0,1</td>\n",
       "      <td>700,00</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>01/12/2023</td>\n",
       "      <td>MI</td>\n",
       "      <td>NXVD219</td>\n",
       "      <td>C</td>\n",
       "      <td>16,9</td>\n",
       "      <td>700,00</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Hora       Fecha Pais   Unidad Tipo Oferta Energ�a Compra/Venta  \\\n",
       "0   1.0  01/12/2023   MI  WMVD204           C                  2,2   \n",
       "1   1.0  01/12/2023   MI  WMVD090           C                  0,1   \n",
       "2   1.0  01/12/2023   MI  NXVD238           C                  0,8   \n",
       "3   1.0  01/12/2023   MI  NXVD253           C                  0,1   \n",
       "4   1.0  01/12/2023   MI  NXVD219           C                 16,9   \n",
       "\n",
       "  Precio Compra/Venta Ofertada (O)/Casada (C)  Unnamed: 8  \n",
       "0            1.002,00                       O         NaN  \n",
       "1            1.002,00                       O         NaN  \n",
       "2              700,00                       O         NaN  \n",
       "3              700,00                       O         NaN  \n",
       "4              700,00                       O         NaN  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame_combinado.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (JAIME MARTIN) In this Section, We are going to clean and prepare Data obtained from various\n",
    "# sources related to the OMIE SPOT Electricity\n",
    "#1 Handly Missing Values\n",
    "#2 Removing Outliers\n",
    "#3 Data Normalization \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
